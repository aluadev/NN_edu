{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Part Of Speech Tagging\n",
    "#We're now going to solve the same problem of POS tagging with neural networks. \n",
    "\n",
    "#From deep learning perspective, this is a task of predicting a sequence of outputs aligned to a sequence of inputs. \n",
    "#There are several problems that match this formulation:\n",
    "#Part Of Speech Tagging - an auxuliary task for many NLP problems\n",
    "#Named Entity Recognition - for chat bots and web crawlers\n",
    "#Protein structure prediction - for bioinformatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/alua/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/alua/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "<ipython-input-3-bc9847c14597>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "    \n",
    "    \n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabularies\n",
    "\n",
    "#Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates \n",
    "#on a word level, processing one word per RNN step. This means we'll have to deal with far larger vocabulary.\n",
    "#Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have \n",
    "#a large vocabulary for free by using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage = 0.92876\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for sentence in data:\n",
    "    words,tags = zip(*sentence)\n",
    "    word_counts.update(words)\n",
    "\n",
    "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n",
    "\n",
    "#let's measure what fraction of data words are in the dictionary\n",
    "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) })\n",
    "tag_to_id = { tag: i for i, tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words and tags into fixed-size matrix\n",
    "def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n",
    "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,lines))\n",
    "    matrix = np.empty([len(lines), max_len],dtype)\n",
    "    matrix.fill(pad)\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
    "        matrix[i,:len(line_ix)] = line_ix\n",
    "\n",
    "    return matrix.T if time_major else matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ids:\n",
      "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
      "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
      "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
      "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
      "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
      "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
      " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
      "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Tag ids:\n",
      "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
      "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
      "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
      "   6  3  2 13  7]\n",
      " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
    "\n",
    "print(\"Word ids:\")\n",
    "print(to_matrix(batch_words, word_to_id))\n",
    "print(\"Tag ids:\")\n",
    "print(to_matrix(batch_tags, tag_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "\n",
    "#Unlike our previous lab, this time we'll focus on a high-level keras interface to recurrent neural networks. \n",
    "#It is as simple as you can get with RNN, allbeit somewhat constraining for complex tasks like seq2seq.\n",
    "#By default, all keras RNNs apply to a whole sequence of inputs and produce a sequence of hidden states \n",
    "#(return_sequences=True or just the last hidden state (return_sequences=False). \n",
    "#All the recurrence is happening under the hood.\n",
    "#At the top of our model we need to apply a Dense layer to each time-step independently. \n",
    "#As of now, by default keras.layers.Dense would apply once to all time-steps concatenated. \n",
    "#We use keras.layers.TimeDistributed to modify Dense layer so that it would apply across both batch and time axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training: in this case we don't want to prepare the whole training dataset in advance. \n",
    "#The main cause is that the length of every batch depends on the maximum sentence length within the batch. \n",
    "#This leaves us two options: use custom training code as in previous seminar or use generators.\n",
    "#Keras models have a model.fit_generator method that accepts a python generator yielding one batch at a time. \n",
    "#But first we need to implement such generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "BATCH_SIZE=32\n",
    "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
    "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is q numpy array\"\n",
    "    \n",
    "    while True:\n",
    "        indices = np.random.permutation(np.arange(len(sentences)))\n",
    "        for start in range(0,len(indices)-1,batch_size):\n",
    "            batch_indices = indices[start:start+batch_size]\n",
    "            batch_words,batch_tags = [],[]\n",
    "            for sent in sentences[batch_indices]:\n",
    "                words,tags = zip(*sent)\n",
    "                batch_words.append(words)\n",
    "                batch_tags.append(tags)\n",
    "\n",
    "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
    "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
    "\n",
    "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
    "            yield batch_words,batch_tags_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks: Another thing we need is to measure model performance. The tricky part is not to count \n",
    "#accuracy after sentence ends (on padding) and making sure we count all the validation data exactly once.\n",
    "#While it isn't impossible to persuade Keras to do all of that, we may as well write our own callback that does that.\n",
    "#Keras callbacks allow you to write a custom code to be ran once every epoch or every minibatch. \n",
    "#We'll define one via LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
    "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
    "\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
    "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
    "\n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
    "    denominator = np.sum(test_words != 0)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.6012\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.94023\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.0604\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.94456\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.0510\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.94662\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.0465\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.94567\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.0421\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 3s 6ms/step\n",
      "\n",
      "Validation accuracy: 0.94533\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x148902e80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 3s 6ms/step\n",
      "Final accuracy: 0.94533\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"Final accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, None, 64)          7360      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 14)          910       \n",
      "=================================================================\n",
      "Total params: 508,370\n",
      "Trainable params: 508,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going bidirectional\n",
    "\n",
    "#Since we're analyzing a full sequence, it's legal for us to look into future data.\n",
    "#A simple way to achieve that is to go both directions at once, making a bidirectional RNN.\n",
    "#In Keras you can achieve that both manually (using two LSTMs and Concatenate) and by using \n",
    "#keras.layers.Bidirectional.\n",
    "#This one works just as TimeDistributed we saw before: you wrap it around a recurrent layer \n",
    "#(SimpleRNN now and LSTM/GRU later) and it actually creates two layers under the hood.\n",
    "#Your first task is to use such a layer our POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model that utilizes bidirectional SimpleRNN\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#add bidirectional layer\n",
    "#add top layer that predicts tag probabilities\n",
    "\n",
    "model.add(L.Bidirectional(L.LSTM(10, return_sequences=True), merge_mode='concat', weights=None, backward_layer=None))\n",
    "model.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn_16 (SimpleRNN)    (None, None, 64)          7360      \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, None, 20)          6000      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, None, 14)          294       \n",
      "=================================================================\n",
      "Total params: 513,754\n",
      "Trainable params: 513,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 39s 27ms/step - loss: 0.7689\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 8s 18ms/step\n",
      "\n",
      "Validation accuracy: 0.94579\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 35s 26ms/step - loss: 0.0689\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 7s 16ms/step\n",
      "\n",
      "Validation accuracy: 0.95453\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 35s 26ms/step - loss: 0.0475\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 7s 17ms/step\n",
      "\n",
      "Validation accuracy: 0.95844\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 37s 28ms/step - loss: 0.0402\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 7s 17ms/step\n",
      "\n",
      "Validation accuracy: 0.95778\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 36s 27ms/step - loss: 0.0359\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 7s 17ms/step\n",
      "\n",
      "Validation accuracy: 0.96011\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dea5970>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 7s 16ms/step\n",
      "\n",
      "Final accuracy: 0.96011\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task I: Structured loss functions (more bonus points)\n",
    "#Since we're tagging the whole sequence at once, we might as well train our network to do so. \n",
    "#Remember linear CRF from the lecture? You can also use it as a loss function for your RNN\n",
    "#There's more than one way to do so, but we'd recommend starting with Conditional Random Fields\n",
    "#You can plug CRF as a loss function and still train by backprop. \n",
    "#Alternatively, you can condition your model on previous tags (make it autoregressive) and perform \n",
    "#beam search over that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "base = model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "model.add(L.Bidirectional(L.LSTM(10, return_sequences=True), merge_mode='concat', weights=None, backward_layer=None))\n",
    "base = model.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages\n"
     ]
    }
   ],
   "source": [
    "from distutils.sysconfig import get_python_lib\n",
    "print(get_python_lib())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CRFLayer' from 'crf' (/opt/anaconda3/lib/python3.8/site-packages/crf/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-ece0fee617a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcrf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRFLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CRFLayer' from 'crf' (/opt/anaconda3/lib/python3.8/site-packages/crf/__init__.py)"
     ]
    }
   ],
   "source": [
    "from crf import CRFLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
